{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["pQECP4q5SdPG","vJ0309b8aQWE","dIRbkpOCbt-P","wOEspzUOcG1Y"],"authorship_tag":"ABX9TyNMDcQUR2S6x30hDY0K1r+c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# CLASIFICADOR DE TEXTO MEDIANTE VOTACION MAYORITARIA TASK 5"],"metadata":{"id":"kEkOKNbjSVfx"}},{"cell_type":"markdown","source":["## Importar Dependencias y Librerias"],"metadata":{"id":"pQECP4q5SdPG"}},{"cell_type":"code","source":["# Instalacion de dependencias\n","!pip install pytorch-lightning\n","!pip install --upgrade accelerate\n","!pip install framework-reproducibility\n","!pip install transformers datasets\n","!pip install --upgrade numpy\n","!pip install --upgrade pandas\n","!pip install --upgrade scikit-learn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rqI4xWNDScR9","executionInfo":{"status":"ok","timestamp":1718814828631,"user_tz":-120,"elapsed":182506,"user":{"displayName":"Alvaro Carrillo","userId":"09121382412307725613"}},"outputId":"4dd55a48-69de-4838-a12e-45c751987132"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorch-lightning\n","  Downloading pytorch_lightning-2.3.0-py3-none-any.whl (812 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.2/812.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.25.2)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.3.0+cu121)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.4)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n","Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n","Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n","  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.1)\n","Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\n","Collecting lightning-utilities>=0.8.0 (from pytorch-lightning)\n","  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2.31.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (67.7.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (3.14.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (3.1.4)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.0.0->pytorch-lightning)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.0.0->pytorch-lightning)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.0.0->pytorch-lightning)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.0.0->pytorch-lightning)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.0.0->pytorch-lightning)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.0.0->pytorch-lightning)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.0.0->pytorch-lightning)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.0.0->pytorch-lightning)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.0.0->pytorch-lightning)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.0.0->pytorch-lightning)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.0.0->pytorch-lightning)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->pytorch-lightning)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->pytorch-lightning) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2024.6.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->pytorch-lightning) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch-lightning\n","Successfully installed lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pytorch-lightning-2.3.0 torchmetrics-1.4.0.post0\n","Collecting accelerate\n","  Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.31.0\n","Collecting framework-reproducibility\n","  Downloading framework_reproducibility-0.6.0-py2.py3-none-any.whl (19 kB)\n","Installing collected packages: framework-reproducibility\n","Successfully installed framework-reproducibility-0.6.0\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n","Collecting datasets\n","  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Collecting pyarrow>=15.0.0 (from datasets)\n","  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Collecting requests (from transformers)\n","  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.31.0\n","    Uninstalling requests-2.31.0:\n","      Successfully uninstalled requests-2.31.0\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 14.0.2\n","    Uninstalling pyarrow-14.0.2:\n","      Successfully uninstalled pyarrow-14.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n","google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-16.1.0 requests-2.32.3 xxhash-3.4.1\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n","Collecting numpy\n","  Downloading numpy-2.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.25.2\n","    Uninstalling numpy-1.25.2:\n","      Successfully uninstalled numpy-1.25.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","astropy 5.3.4 requires numpy<2,>=1.21, but you have numpy 2.0.0 which is incompatible.\n","cudf-cu12 24.4.1 requires numpy<2.0a0,>=1.23, but you have numpy 2.0.0 which is incompatible.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n","cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.0.0 which is incompatible.\n","ibis-framework 8.0.0 requires numpy<2,>=1, but you have numpy 2.0.0 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\n","numba 0.58.1 requires numpy<1.27,>=1.22, but you have numpy 2.0.0 which is incompatible.\n","rmm-cu12 24.4.0 requires numpy<2.0a0,>=1.23, but you have numpy 2.0.0 which is incompatible.\n","scipy 1.11.4 requires numpy<1.28.0,>=1.21.6, but you have numpy 2.0.0 which is incompatible.\n","tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-2.0.0\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","Collecting pandas\n","  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.0.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Installing collected packages: pandas\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 2.0.3\n","    Uninstalling pandas-2.0.3:\n","      Successfully uninstalled pandas-2.0.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires numpy<2.0a0,>=1.23, but you have numpy 2.0.0 which is incompatible.\n","cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 2.2.2 which is incompatible.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n","google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 2.2.2 which is incompatible.\n","google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n","ibis-framework 8.0.0 requires numpy<2,>=1, but you have numpy 2.0.0 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed pandas-2.2.2\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Collecting scikit-learn\n","  Downloading scikit_learn-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (2.0.0)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n","Collecting numpy>=1.19.5 (from scikit-learn)\n","  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy, scikit-learn\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.0\n","    Uninstalling numpy-2.0.0:\n","      Successfully uninstalled numpy-2.0.0\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.2.2\n","    Uninstalling scikit-learn-1.2.2:\n","      Successfully uninstalled scikit-learn-1.2.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 2.2.2 which is incompatible.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.26.4 scikit-learn-1.5.0\n"]}]},{"cell_type":"markdown","source":["Reiniciar Entorno (Recomendable de hacer siempre despues de instalar dependencias)"],"metadata":{"id":"_XEeFKV0Swu6"}},{"cell_type":"code","source":["# Instalacion de librerias\n","import random\n","import torch\n","import numpy as np\n","import os\n","from pytorch_lightning import seed_everything\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import re\n","\n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)# Store the average loss after eachepoch so we can plot them.\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\" # See:https://github.com/NVIDIA/tensorflow-determinism#confirmed-current-gpu-specific-sources-of-non-determinism-with-solutions\n","seed_everything(42, workers=True)\n","\n","from datasets import Dataset, DatasetDict, load_metric\n","import pandas as pd\n","import sklearn as sk\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score, f1_score\n","from sklearn.model_selection import train_test_split\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, \\\n","TrainingArguments, Trainer, pipeline, EarlyStoppingCallback"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Vzvy6HlS6ai","executionInfo":{"status":"ok","timestamp":1718814916594,"user_tz":-120,"elapsed":25701,"user":{"displayName":"Alvaro Carrillo","userId":"09121382412307725613"}},"outputId":"39516b8c-258e-4d90-8b18-09097dd04c2d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:lightning_fabric.utilities.seed:Seed set to 42\n"]}]},{"cell_type":"code","source":["# Comprobacion GPU\n","# Check that pyTorch is identifying the GPU\n","if torch.cuda.device_count() > 0:\n","    # If a GPU is available, print its name\n","    print(f'GPU detected. Currently using: \"{torch.cuda.get_device_name(0)}\"')\n","    # Set the device to GPU for accelerated computations\n","    device = torch.device(\"cuda\")\n","else:\n","    # If no GPU is available, inform the user to change the runtime type\n","    print('Currently using CPU. To utilize GPU acceleration, change the runtime type in the \\'runtime\\' tab.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qvjihweYTADL","executionInfo":{"status":"ok","timestamp":1718814955929,"user_tz":-120,"elapsed":266,"user":{"displayName":"Alvaro Carrillo","userId":"09121382412307725613"}},"outputId":"87b78912-de93-457b-a695-34b3bf17e347"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Currently using CPU. To utilize GPU acceleration, change the runtime type in the 'runtime' tab.\n"]}]},{"cell_type":"code","source":["# Conexion drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I3Ch2uWoTYOm","executionInfo":{"status":"ok","timestamp":1718814977876,"user_tz":-120,"elapsed":18331,"user":{"displayName":"Alvaro Carrillo","userId":"09121382412307725613"}},"outputId":"06c48bfb-4c25-46d3-ec7b-d960f5dbc04a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Lectura y Etiquetado de Datos"],"metadata":{"id":"JhzEpPNUT423"}},{"cell_type":"markdown","source":["Cargar los datos de entrenamiento y test"],"metadata":{"id":"L3F9PCLOZOVm"}},{"cell_type":"code","source":["# En este caso, tenemos un único fichero de entrenamiento y un fichero independiente de test\n","train_data_path = '/content/drive/MyDrive/Dataset/Task 5/train_original.json'\n","test_data_path = '/content/drive/MyDrive/Dataset/Task 5/test_task5_hard.json'\n","#############################################################################################\n","\n","# Los transformamos en Dataframes\n","train_df_full = pd.read_json(train_data_path, orient='index')\n","test_df = pd.read_json(test_data_path, orient='index')"],"metadata":{"id":"hONVaypSUF-r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hard voting train_df"],"metadata":{"id":"-7fHXhPrZKdx"}},{"cell_type":"code","source":["train_df_full.reset_index(drop=True, inplace=True)\n","\n","# Cuenta la etiqueta más usada\n","def most_common_label(labels):\n","    counts = {}\n","    for label in labels:\n","        counts[label] = counts.get(label, 0) + 1\n","    return max(counts, key=counts.get)\n","\n","columna_labels_task5 = train_df_full['labels_task5']\n","mv = []\n","indices = []\n","i = 0\n","\n","for columna in columna_labels_task5:\n","    labels = []\n","    for data in columna:\n","        if data == 'DIRECT':\n","            labels.append('DIRECT')\n","        elif data == 'JUDGEMENTAL':\n","            labels.append('JUDGEMENTAL')\n","    if labels != []:\n","        mas_usado = most_common_label(labels)\n","        mv.append(mas_usado)\n","        indices.append(i)\n","    i += 1\n","\n","train_df_full = train_df_full.loc[indices]\n","train_df_full['labels_task5'] = mv\n","train_df_full"],"metadata":{"id":"ME5aMz2UUsUu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Balanceo de datos y divison train-valid"],"metadata":{"id":"e3ScgLhRY-AO"}},{"cell_type":"code","source":["# Usamos estas variables para que el código sea más portable\n","nombre_etiqueta = 'labels_task5'\n","\n","# Muestra la distribucón original de las etiquetas\n","print(\"Distribución original - Train completo: \", train_df_full.value_counts(nombre_etiqueta))\n","\n","######## Undersampling manual ########################\n","# Para hacer un undersampling manual, se construye un dataframe para cada clase\n","# Por ejemplo, si se quiere hacer undersampling de la clase mayoritaria (0), se guarda\n","# en df_0 el número de filas de clase 0 que se quiere mantener y en df_1 todas las filas de clase 1\n","\n","# Contar cuántos son de cada clase y coger el minimo\n","num_class1 = (train_df_full[nombre_etiqueta] == 'DIRECT').sum()\n","num_class2 = (train_df_full[nombre_etiqueta] == 'JUDGEMENTAL').sum()\n","min_size = min(num_class1,num_class2)\n","\n","# *******\n","df_0 = train_df_full[train_df_full[nombre_etiqueta]=='DIRECT'].sample(n=min_size,random_state=42)\n","df_1 = train_df_full[train_df_full[nombre_etiqueta]=='JUDGEMENTAL'].sample(n=min_size,random_state=42)\n","# Se vuelve a construir el fichero de entrenamiento concatenando los 2 dataframes\n","train_df_full = pd.concat([df_0,df_1])\n","print(\"Distribución despues del undersampling: \", train_df_full.value_counts(nombre_etiqueta))\n","######################################################\n","# *******\n","\n","###### División train/valid/test #####################\n","# Si hay un único fichero\n","train_df, valid_df = train_test_split(train_df_full, test_size = 0.15, shuffle = True, stratify=train_df_full[[nombre_etiqueta]])\n","#valid_df, test_df = train_test_split(auxiliar_df, test_size = 0.3, shuffle = True, stratify=auxiliar_df[[nombre_etiqueta]])\n","\n","# Si hay ficheros de train y test independientes, sólo se hace división train/valid\n","train_df, valid_df = train_test_split(train_df_full, test_size = 0.15, shuffle = True, stratify=train_df_full[[nombre_etiqueta]])\n","######################################################\n","\n","\n","print(\"Ejemplos del conjunto completo de entrenamiento \", len(train_df_full))\n","print(\"Ejemplos usados para entrenar: \", len(train_df))\n","print(\"Ejemplos usados para validar: \", len(valid_df))\n","print(\"Ejemplos usados para test: \", len(test_df))"],"metadata":{"id":"JexDD99cYybt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Para saber el número de filas de cada clase en cada división\n","print(\"distribución original - Train: \",train_df.value_counts(nombre_etiqueta))\n","print(\"distribución original - Valid: \",valid_df.value_counts(nombre_etiqueta))\n","print(\"distribución original - Test: \",test_df.value_counts(nombre_etiqueta))"],"metadata":{"id":"kW0nueYpaHj4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preprocesado de Datos"],"metadata":{"id":"vJ0309b8aQWE"}},{"cell_type":"markdown","source":["Funciones de limpieza"],"metadata":{"id":"Xn3hA-KeaWbh"}},{"cell_type":"code","source":["import re\n","\n","def remove_links(tweet):\n","    \"\"\"Takes a string and removes web links from it\"\"\"\n","    tweet = re.sub(r'http\\S+', '', tweet)        # remove http links\n","    tweet = re.sub(r'bit.ly/\\S+', '', tweet)     # remove bitly links\n","    tweet = re.sub(r'\\[link\\]', '', tweet )      # remove [link]\n","    tweet = re.sub(r'\\[url\\]', '', tweet )       # remove [url]\n","    tweet = re.sub(r'pic.twitter\\S+','', tweet)\n","    return tweet\n","\n","def remove_users(tweet):\n","    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n","    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove re-tweet\n","    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)      # remove tweeted at\n","    tweet = re.sub(r'\\[user\\]', '', tweet )                      # remove [user]\n","    return tweet\n","\n","def remove_hashtags(tweet):\n","    \"\"\"Takes a string and removes any hash tags\"\"\"\n","    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)      # remove hash tags\n","    return tweet\n","\n","def remove_av(tweet):\n","    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n","    tweet = re.sub('VIDEO:', '', tweet)  # remove 'VIDEO:' from start of tweet\n","    tweet = re.sub('AUDIO:', '', tweet)  # remove 'AUDIO:' from start of tweet\n","    return tweet\n","\n","def remove_emojis(tweet):\n","    emoj = re.compile(\"[\"\n","        u\"\\U00002700-\\U000027BF\"  # Dingbats\n","        u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n","        u\"\\U00002600-\\U000026FF\"  # Miscellaneous Symbols\n","        u\"\\U0001F300-\\U0001F5FF\"  # Miscellaneous Symbols And Pictographs\n","        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n","        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n","        u\"\\U00010000-\\U0010FFFF\"\n","        u\"\\U0001F680-\\U0001F6FF\"  # Transport and Map Symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","        u\"\\U0001f926-\\U0001f937\"\n","        u\"\\U00010000-\\U0010ffff\"\n","        u\"\\u2640-\\u2642\"\n","        u\"\\u2600-\\u2B55\"\n","        u\"\\ufe0f\"  # dingbats\n","\n","                      \"]+\", re.UNICODE)\n","    return re.sub(emoj, '', tweet)"],"metadata":{"id":"DGavid9uaaAv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Aplicación de las funciones"],"metadata":{"id":"1kUxr7Xdah9h"}},{"cell_type":"code","source":["campo_texto = 'text'\n","\n","train_df[campo_texto] = train_df[campo_texto].str.lower()\n","valid_df[campo_texto] = valid_df[campo_texto].str.lower()\n","test_df[campo_texto] = test_df[campo_texto].str.lower()\n","\n","#train_df[campo_texto] = train_df[campo_texto].apply(remove_links)\n","#valid_df[campo_texto] = valid_df[campo_texto].apply(remove_links)\n","#test_df[campo_texto] = test_df[campo_texto].apply(remove_links)\n","\n","#train_df[campo_texto] = train_df[campo_texto].apply(remove_users)\n","#valid_df[campo_texto] = valid_df[campo_texto].apply(remove_users)\n","#test_df[campo_texto] = test_df[campo_texto].apply(remove_users)\n","\n","#train_df[campo_texto] = train_df[campo_texto].apply(remove_hashtags)\n","#valid_df[campo_texto] = valid_df[campo_texto].apply(remove_hashtags)\n","#test_df[campo_texto] = test_df[campo_texto].apply(remove_hashtags)\n","\n","#train_df[campo_texto] = train_df[campo_texto].apply(remove_emojis)\n","#valid_df[campo_texto] = valid_df[campo_texto].apply(remove_emojis)\n","#test_df[campo_texto] = test_df[campo_texto].apply(remove_emojis)\n","\n","train_df"],"metadata":{"id":"IBUKNWhHamRC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Formateo y Etiquetado de los Datos"],"metadata":{"id":"2KLQvEtGawss"}},{"cell_type":"code","source":["# Se convierten los dataframes en objetos Datasets para que los acepten los Rransformers\n","train_dataset = Dataset.from_pandas(train_df)\n","valid_dataset = Dataset.from_pandas(valid_df)\n","test_dataset = Dataset.from_pandas(test_df)\n","\n","print(train_dataset, valid_dataset, test_dataset)"],"metadata":{"id":"cCCr3EKSa7Su"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Los objetos de tipo Dataset también se pueden mostrar en formato pandas\n","train_dataset.set_format(\"pandas\")\n","train_dataset[:]"],"metadata":{"id":"oPFlsfXJbAPi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reseteamos el formato para que evitar posibles fallos\n","train_dataset.reset_format()\n","valid_dataset.reset_format()"],"metadata":{"id":"UcsQG8cVbEBn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Esta función toma un registro como entrada, que contiene una etiqueta llamada 'label'.\n","# Si el valor de esta etiqueta es 0, asigna 0 a la variable 'label'. Si el valor no es 0\n","# asigna 1 a 'label'. A continuación, la función devuelve un diccionario con la etiqueta modificada, llamado \"labels\"\n","\n","def set_labels(records):\n","  if records[nombre_etiqueta] == 'JUDGEMENTAL':\n","    label = 0\n","  else:\n","    label = 1\n","  return {'labels': label}"],"metadata":{"id":"2fWGf07dbXxS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Mapeado de la Función"],"metadata":{"id":"dIRbkpOCbt-P"}},{"cell_type":"code","source":["# Aplicamos la función a cada fila de los conjuntos de entrenamiento y validación\n","train_dataset = train_dataset.map(set_labels)\n","valid_dataset = valid_dataset.map(set_labels)\n","\n","print(train_dataset, valid_dataset)"],"metadata":{"id":"zM9ry-rAbdFR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reseteamos el formato para que no haya fallos\n","train_dataset.reset_format()\n","valid_dataset.reset_format()\n","test_dataset.reset_format()"],"metadata":{"id":"emkBsQtcb7DR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Selección de Modelo"],"metadata":{"id":"wOEspzUOcG1Y"}},{"cell_type":"code","source":["# Define the model checkpoint to be used for the task.\n","# Uncomment the desired model_checkpoint or replace it with your own.\n","\n","#model_checkpoint = 'xlm-roberta-base' # This model is one of the top-performing models in our experiments por the Spanish dataset\n","model_checkpoint = 'bert-base-multilingual-uncased'"],"metadata":{"id":"hlQsga38cM7o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Tokenización"],"metadata":{"id":"Mml3DAL7cCBP"}},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_auth_token='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')"],"metadata":{"id":"d6kp1cUMcExv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Función para tokenizar un dataset\n","# La función tokenizer() hace la tokenización y devuelve los 'inputs_id' y los 'attention_mask'\n","\n","# Definir el método que se asignará al conjunto de datos para tokenizar los datos.\n","# Esta función toma un diccionario 'examples' como entrada, que contiene una clave llamada 'campo_texto'.\n","# La función usa el tokenizer para tokenizar el texto, lo trunca si excede la longitud máxima (MAX_LENGTH),\n","# y lo rellena para asegurar que todas las secuencias tienen la misma longitud.\n","\n","def tokenize_data(examples):\n","  return tokenizer(examples[campo_texto], truncation=True, max_length=MAX_LENGTH, padding=True)"],"metadata":{"id":"1UKnMvxWcZgs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MAX_LENGTH = 128\n","\n","# Construción de los ficheros codificados (encoded)\n","columns_train = train_dataset.column_names  # Coge todas las columnas\n","columns_valid = valid_dataset.column_names  # Coge todas las columnas\n","columns_train.remove(\"labels\") # Elimina la columna \"labels\"\n","columns_valid.remove(\"labels\") # Elimina la columna \"labels\"\n","\n","\n","# Hace la tokenización y elimina todas las columnas que no se necesitan\n","encoded_train_dataset = train_dataset.map(tokenize_data, batched=True, remove_columns=columns_train)\n","encoded_valid_dataset = valid_dataset.map(tokenize_data, batched=True, remove_columns=columns_valid)\n","encoded_train_dataset[100]"],"metadata":{"id":"-S2FHESGcdM2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_df.reset_index(drop=True, inplace=True)\n","train_df.loc[0]"],"metadata":{"id":"VKo7eK-cchLe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Definición de Métricas"],"metadata":{"id":"PnZnwW_5ckK0"}},{"cell_type":"code","source":["# Función para realizar distintas métricas en ejecución\n","\n","def compute_metrics(eval_pred):\n","\n","  ##############\n","  ## predictions son logits, que son tuplas de la forma [valor1, valor2]\n","  ## Por ejemplo [-1.5606991,  1.6122842] significa que ha predicho eso para un documento\n","  ## Eso es lo que pasa a la última capa del transformer (softmax si es binario)\n","  ## Por eso se utiliza el índice del valor máximo de la tupla, para decir que esa es la clase que predice\n","\n","  ## label_ids = [0, 1, 1, 0, 1]  # Etiquetas reales\n","  ## predictions = [\n","  ##  [0.8, 0.2],  # Predicciones para la primera instancia\n","  ##  [0.3, 0.7],  # Predicciones para la segunda instancia\n","  ##  [0.1, 0.9],  # Predicciones para la tercera instancia\n","  ##  [0.9, 0.1],  # Predicciones para la cuarta instancia\n","  ##  [0.4, 0.6],  # Predicciones para la quinta instancia\n","  ##           ]\n","\n","  ##############\n","\n","  labels = eval_pred.label_ids\n","  preds = eval_pred.predictions.argmax(-1)\n","\n","  # Compute precision, recall, F1-score, and support\n","  precision, recall, f1, _ = sk.metrics.precision_recall_fscore_support(labels, preds, average=\"macro\")\n","\n","  # Calculate F1-score for the minority class (label = 1)\n","  f1_minoritaria= f1_score(labels, preds, pos_label=1)\n","\n","  # Calculate F1-score for the majority class (label = 0)\n","  f1_mayoritaria = f1_score(labels, preds, pos_label=0)\n","\n","  # Calculate accuracy\n","  acc = sk.metrics.accuracy_score(labels, preds)\n","\n","  # Calculate Area Under the Curve (AUC)\n","  AUC = roc_auc_score(labels, preds)\n","\n","  # Calculate Precision-Recall Area Under the Curve (AUC)\n","  PREC_REC = average_precision_score(labels, preds)\n","\n","  return {\n","      'accuracy': acc,\n","      'f1': f1,\n","      'precision': precision,\n","      'recall': recall,\n","      'AUC': AUC,\n","      'f1_minoritaria': f1_minoritaria,\n","      'f1_mayoritaria': f1_mayoritaria,\n","      'PREC_REC': PREC_REC\n","  }"],"metadata":{"id":"kGyqeyKUcnEt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Entrenamiento del Modelo"],"metadata":{"id":"LY8NgExbcqHp"}},{"cell_type":"code","source":["# Se carga el modelo preentrenado\n","n_labels = 2\n","\n","# El uso de una función de inicialización facilita la repetición del entrenamiento\n","# Se puede usar la misma función de inicialización en diferentes ejecuciones del código o en configuraciones de entrenamiento diferentes\n","# Esto facilita la repetición del entrenamiento y la reproducibilidad, ya que se puede inicializar el modelo\n","# de la misma manera en cada ejecución.\n","\n","def model_init():\n","    return AutoModelForSequenceClassification.from_pretrained(model_checkpoint,\n","                                                              num_labels = n_labels) #, return_dict = True )\n","                                                              # use_auth_token = 'token propio de HugginFace')"],"metadata":{"id":"Jef8CACGcwej"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Para saber el nombre del modelo\n","model_name = model_checkpoint.split(\"/\")[-1]\n","model_name"],"metadata":{"id":"C7Kq33kgcz2S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fine-tuning"],"metadata":{"id":"HrXKRS2uc1xQ"}},{"cell_type":"code","source":["# Selección de hiperparámetros\n","BATCH_SIZE = 32\n","NUM_TRAIN_EPOCHS = 15\n","LEARNING_RATE = 3e-5\n","MAX_LENGTH = 128\n","WEIGHT_DECAY = 0.1"],"metadata":{"id":"5Pdcw_s4c4SQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Se definen los parámetros del Trainer()\n","def maximum(a, b):\n","    if a >= b:\n","        return a\n","    else:\n","        return b\n","\n","\n","num_train_samples = int(len(encoded_train_dataset))\n","num_evaluation= int(len(encoded_valid_dataset))\n","\n","value = len(encoded_train_dataset) // (2 * BATCH_SIZE * NUM_TRAIN_EPOCHS)\n","logging_steps = maximum(1, value)\n","\n","# logging_steps = max(1,len(encoded_train_dataset) // (2 * BATCH_SIZE * NUM_TRAIN_EPOCHS))\n","\n","optim = [\"adamw_hf\", \"adamw_torch\", \"adamw_apex_fused\", \"adafactor\", \"adamw_torch_xla\"]\n","\n","training_args = TrainingArguments(\n","    output_dir = 'results',\n","    num_train_epochs = NUM_TRAIN_EPOCHS,\n","    learning_rate = LEARNING_RATE,\n","    per_device_train_batch_size = BATCH_SIZE,\n","    per_device_eval_batch_size = BATCH_SIZE,\n","    load_best_model_at_end = True,\n","    metric_for_best_model = 'f1', # Cambiar la metrica por la que queremos ajustar\n","    #metric_for_best_model = 'eval_loss',\n","    weight_decay = WEIGHT_DECAY,\n","    evaluation_strategy = 'epoch',\n","    save_strategy = 'epoch',\n","    #logging_steps = logging_steps,\n","    save_total_limit = 3,\n","    optim = optim[1],\n","    push_to_hub = False\n",")"],"metadata":{"id":"VjWz_8yCc8pD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Se crea el objeto Trainer()\n","trainer = Trainer(\n","    model_init = model_init,\n","    args = training_args,\n","    compute_metrics = compute_metrics,\n","    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)],\n","    train_dataset = encoded_train_dataset,\n","    eval_dataset = encoded_valid_dataset,\n","    tokenizer = tokenizer\n",")"],"metadata":{"id":"0JPOpYeRdKko"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A entrenar\n","trainer.train()"],"metadata":{"id":"tGoMkEN9dLpb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluación del Modelo"],"metadata":{"id":"juuMrKekdPiG"}},{"cell_type":"markdown","source":["Durante validación"],"metadata":{"id":"rhexgs3xdUQv"}},{"cell_type":"code","source":["eval = trainer.evaluate()\n","# Se pasa el resultado a Dataframe\n","dfeval = pd.DataFrame(list(eval.items()), columns = ['Nombre','Valor'])\n","dfeval"],"metadata":{"id":"umeFnhdBdWBr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Se graba el modelo entrenado\n","trainer.save_model('/home/alvarocarrillo/TFG/Trabajo/Dataset/Modelos/Modelo_Roberta')"],"metadata":{"id":"Tsox9jymdgmO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Evaluación con el test"],"metadata":{"id":"Z_zhXoMSdiJd"}},{"cell_type":"code","source":["# Lo pasamos a objeto dataset\n","test_dataset = Dataset.from_pandas(test_df)\n","test_dataset"],"metadata":{"id":"flW_6V50dko1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### SOLO CUANDO ESTAMOS EVALUANDO UN TEST ETIQUETADO\n","# Pasamos la etiqueta a label y le damos formato numérico\n","test_dataset = test_dataset.map(set_labels)  # La función set_labels ya se definió en el entrenamiento\n","test_dataset"],"metadata":{"id":"JQdXk8Amdotq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Predicciones"],"metadata":{"id":"0fCI2W_Ldu71"}},{"cell_type":"code","source":["# Se carga el modelo que se ha entrenado\n","model_path = '/home/alvarocarrillo/TFG/Trabajo/Dataset/Modelos/Modelo_Roberta'\n","\n","model = AutoModelForSequenceClassification.from_pretrained(model_path)"],"metadata":{"id":"k03vpBzbdt4V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Predicción con pipeline\n","pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=0)"],"metadata":{"id":"KtsWbiEEd1jc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hacemos las prediciones\n","def get_predictions(records):\n","  result = pipe(records[campo_texto], truncation=True)\n","  pred_label = result[0]['label']\n","  score_label = result[0]['score']\n","  #print(pred_label)\n","\n","  if pred_label == 'LABEL_0':\n","    pred_label = 0\n","  else:\n","    pred_label = 1\n","\n","  return {'pred_label': pred_label, 'score_label': score_label}"],"metadata":{"id":"eAzRW_1rd3hT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Se hacen las predicciones sobre el conjunto de test\n","test_dataset_predicted = test_dataset.map(get_predictions)\n","test_dataset_predicted[0]"],"metadata":{"id":"viVwKU25d7cZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataset_predicted.set_format('pandas')\n","df_test = test_dataset_predicted[:]\n","df_test"],"metadata":{"id":"9GZpyYGCd-yE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Añadimos la función de evaluación\n","def compute_metrics(pred):\n","  labels = pred[0]\n","  preds = pred[1]\n","  precision, recall, f1, _ = sk.metrics.precision_recall_fscore_support(labels, preds, average=\"macro\")\n","  acc = sk.metrics.accuracy_score(labels, preds)\n","  AUC = roc_auc_score(labels, preds)\n","  PREC_REC = average_precision_score(labels, preds)\n","  return { 'accuracy': acc, 'f1': f1, 'precision': precision,\n","          'recall': recall, 'AUC': AUC, 'PREC_REC': PREC_REC }"],"metadata":{"id":"5ianP85heF-q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert the pandas series to python list to apply the compute_metric function\n","test_labels = df_test['labels'].values.tolist()\n","test_predictions = df_test['pred_label'].values.tolist()\n","eval_pred_test = [test_labels, test_predictions]"],"metadata":{"id":"reGkHFEQeHdJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["p_test = compute_metrics(eval_pred_test)\n","dftest = pd.DataFrame([[key, p_test[key]] for key in p_test.keys()], columns=['Name', 'Value'])\n","dftest"],"metadata":{"id":"wkhcu5fjeN73"},"execution_count":null,"outputs":[]}]}