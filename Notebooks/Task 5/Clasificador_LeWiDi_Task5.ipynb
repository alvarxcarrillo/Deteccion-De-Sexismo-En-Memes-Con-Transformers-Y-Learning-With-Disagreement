{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["pQECP4q5SdPG"],"authorship_tag":"ABX9TyPORrzZf0yE0POCr6Ef+FJT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# CLASIFICADOR DE TEXTO MEDIANTE ENFOQUE LEARNING WITH DISAGREEMENT TASK 5"],"metadata":{"id":"kEkOKNbjSVfx"}},{"cell_type":"markdown","source":["## Importar Dependencias y Librerias"],"metadata":{"id":"pQECP4q5SdPG"}},{"cell_type":"code","source":["# Instalacion de dependencias\n","!pip install pytorch-lightning\n","!pip install --upgrade accelerate\n","!pip install framework-reproducibility\n","!pip install transformers datasets\n","!pip install --upgrade numpy\n","!pip install --upgrade pandas\n","!pip install --upgrade scikit-learn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rqI4xWNDScR9","executionInfo":{"status":"ok","timestamp":1718814828631,"user_tz":-120,"elapsed":182506,"user":{"displayName":"Alvaro Carrillo","userId":"09121382412307725613"}},"outputId":"4dd55a48-69de-4838-a12e-45c751987132"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorch-lightning\n","  Downloading pytorch_lightning-2.3.0-py3-none-any.whl (812 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.2/812.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.25.2)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.3.0+cu121)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.4)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n","Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n","Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n","  Downloading torchmetrics-1.4.0.post0-py3-none-any.whl (868 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m868.8/868.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.1)\n","Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\n","Collecting lightning-utilities>=0.8.0 (from pytorch-lightning)\n","  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2.31.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.9.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (67.7.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (3.14.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (3.1.4)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.0.0->pytorch-lightning)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.0.0->pytorch-lightning)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.0.0->pytorch-lightning)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.0.0->pytorch-lightning)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.0.0->pytorch-lightning)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.0.0->pytorch-lightning)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.0.0->pytorch-lightning)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.0.0->pytorch-lightning)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.0.0->pytorch-lightning)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.0.0->pytorch-lightning)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.0.0->pytorch-lightning)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch-lightning) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->pytorch-lightning)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->pytorch-lightning) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2024.6.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->pytorch-lightning) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch-lightning\n","Successfully installed lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pytorch-lightning-2.3.0 torchmetrics-1.4.0.post0\n","Collecting accelerate\n","  Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.31.0\n","Collecting framework-reproducibility\n","  Downloading framework_reproducibility-0.6.0-py2.py3-none-any.whl (19 kB)\n","Installing collected packages: framework-reproducibility\n","Successfully installed framework-reproducibility-0.6.0\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n","Collecting datasets\n","  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n","Collecting pyarrow>=15.0.0 (from datasets)\n","  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Collecting requests (from transformers)\n","  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting xxhash (from datasets)\n","  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.31.0\n","    Uninstalling requests-2.31.0:\n","      Successfully uninstalled requests-2.31.0\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 14.0.2\n","    Uninstalling pyarrow-14.0.2:\n","      Successfully uninstalled pyarrow-14.0.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n","google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-16.1.0 requests-2.32.3 xxhash-3.4.1\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n","Collecting numpy\n","  Downloading numpy-2.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.25.2\n","    Uninstalling numpy-1.25.2:\n","      Successfully uninstalled numpy-1.25.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","astropy 5.3.4 requires numpy<2,>=1.21, but you have numpy 2.0.0 which is incompatible.\n","cudf-cu12 24.4.1 requires numpy<2.0a0,>=1.23, but you have numpy 2.0.0 which is incompatible.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n","cupy-cuda12x 12.2.0 requires numpy<1.27,>=1.20, but you have numpy 2.0.0 which is incompatible.\n","ibis-framework 8.0.0 requires numpy<2,>=1, but you have numpy 2.0.0 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\n","numba 0.58.1 requires numpy<1.27,>=1.22, but you have numpy 2.0.0 which is incompatible.\n","rmm-cu12 24.4.0 requires numpy<2.0a0,>=1.23, but you have numpy 2.0.0 which is incompatible.\n","scipy 1.11.4 requires numpy<1.28.0,>=1.21.6, but you have numpy 2.0.0 which is incompatible.\n","tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-2.0.0\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n","Collecting pandas\n","  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.0.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Installing collected packages: pandas\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 2.0.3\n","    Uninstalling pandas-2.0.3:\n","      Successfully uninstalled pandas-2.0.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires numpy<2.0a0,>=1.23, but you have numpy 2.0.0 which is incompatible.\n","cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 2.2.2 which is incompatible.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n","google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 2.2.2 which is incompatible.\n","google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n","ibis-framework 8.0.0 requires numpy<2,>=1, but you have numpy 2.0.0 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed pandas-2.2.2\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Collecting scikit-learn\n","  Downloading scikit_learn-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (2.0.0)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n","Collecting numpy>=1.19.5 (from scikit-learn)\n","  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy, scikit-learn\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.0\n","    Uninstalling numpy-2.0.0:\n","      Successfully uninstalled numpy-2.0.0\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.2.2\n","    Uninstalling scikit-learn-1.2.2:\n","      Successfully uninstalled scikit-learn-1.2.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 2.2.2 which is incompatible.\n","cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n","ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.26.4 scikit-learn-1.5.0\n"]}]},{"cell_type":"markdown","source":["Reiniciar Entorno (Recomendable de hacer siempre despues de instalar dependencias)"],"metadata":{"id":"_XEeFKV0Swu6"}},{"cell_type":"code","source":["# Instalacion de librerias\n","import random\n","import torch\n","import numpy as np\n","import os\n","from pytorch_lightning import seed_everything\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import re\n","\n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)# Store the average loss after eachepoch so we can plot them.\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\" # See:https://github.com/NVIDIA/tensorflow-determinism#confirmed-current-gpu-specific-sources-of-non-determinism-with-solutions\n","seed_everything(42, workers=True)\n","\n","from datasets import Dataset, DatasetDict, load_metric\n","import pandas as pd\n","import sklearn as sk\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score, f1_score\n","from sklearn.model_selection import train_test_split\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, \\\n","TrainingArguments, Trainer, pipeline, EarlyStoppingCallback"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Vzvy6HlS6ai","executionInfo":{"status":"ok","timestamp":1718814916594,"user_tz":-120,"elapsed":25701,"user":{"displayName":"Alvaro Carrillo","userId":"09121382412307725613"}},"outputId":"39516b8c-258e-4d90-8b18-09097dd04c2d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:lightning_fabric.utilities.seed:Seed set to 42\n"]}]},{"cell_type":"code","source":["# Comprobacion GPU\n","# Check that pyTorch is identifying the GPU\n","if torch.cuda.device_count() > 0:\n","    # If a GPU is available, print its name\n","    print(f'GPU detected. Currently using: \"{torch.cuda.get_device_name(0)}\"')\n","    # Set the device to GPU for accelerated computations\n","    device = torch.device(\"cuda\")\n","else:\n","    # If no GPU is available, inform the user to change the runtime type\n","    print('Currently using CPU. To utilize GPU acceleration, change the runtime type in the \\'runtime\\' tab.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qvjihweYTADL","executionInfo":{"status":"ok","timestamp":1718814955929,"user_tz":-120,"elapsed":266,"user":{"displayName":"Alvaro Carrillo","userId":"09121382412307725613"}},"outputId":"87b78912-de93-457b-a695-34b3bf17e347"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Currently using CPU. To utilize GPU acceleration, change the runtime type in the 'runtime' tab.\n"]}]},{"cell_type":"code","source":["# Conexion drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I3Ch2uWoTYOm","executionInfo":{"status":"ok","timestamp":1718814977876,"user_tz":-120,"elapsed":18331,"user":{"displayName":"Alvaro Carrillo","userId":"09121382412307725613"}},"outputId":"06c48bfb-4c25-46d3-ec7b-d960f5dbc04a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Lectura y Etiquetado de Datos"],"metadata":{"id":"JhzEpPNUT423"}},{"cell_type":"markdown","source":["Cargar los datos de entrenamiento y test"],"metadata":{"id":"L3F9PCLOZOVm"}},{"cell_type":"code","source":["# En este caso, tenemos un único fichero de entrenamiento y un fichero independiente de test\n","train_data_path = '/content/drive/MyDrive/Dataset/Task 5/train_original.json'\n","train_data_path = '/content/drive/MyDrive/Dataset/Task 5/train_simple.json'\n","train_data_path = '/content/drive/MyDrive/Dataset/Task 5/train_back.json'\n","test_data_path = '/content/drive/MyDrive/Dataset/Task 5/test_task5_hard.json'\n","#############################################################################################\n","\n","# Los transformamos en Dataframes\n","dataset_txt_full = pd.read_json(train_data_path, orient='index')\n","dataset_txt_full"],"metadata":{"id":"hONVaypSUF-r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Split de los datos"],"metadata":{"id":"BHmQ8rJ81ANx"}},{"cell_type":"code","source":["# Dividir los datos entre train, val y test\n","df = dataset_txt_full.sample(n=int(len(dataset_txt_full)), random_state=42)\n","\n","train_df, val_df = train_test_split(df, test_size=0.15, shuffle=True, stratify=None, random_state=42)\n","\n","print(\"Ejemplos del conjunto completo de entrenamiento: \", len(dataset_txt_full))\n","print(\"Ejemplos usados para entrenar: \", len(train_df))\n","print(\"Ejemplos usados para validar: \", len(val_df))"],"metadata":{"id":"2zJyWDPG1B9q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["val_df.reset_index(drop=True, inplace=True)\n","val_df"],"metadata":{"id":"d-Kj2SCh1Cgh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hard voting val_df"],"metadata":{"id":"-7fHXhPrZKdx"}},{"cell_type":"code","source":["# Cuenta la etiqueta más usada\n","def most_common_label(labels):\n","    counts = {}\n","    for label in labels:\n","        counts[label] = counts.get(label, 0) + 1\n","    return max(counts, key=counts.get)\n","\n","columna_labels_task5 = val_df['labels_task5']\n","mv = []\n","indices = []\n","i = 0\n","\n","for columna in columna_labels_task5:\n","    labels = []\n","    for data in columna:\n","        if data == 'DIRECT':\n","            labels.append('DIRECT')\n","        elif data == 'JUDGEMENTAL':\n","            labels.append('JUDGEMENTAL')\n","    if labels != []:\n","        mas_usado = most_common_label(labels)\n","        mv.append(mas_usado)\n","        indices.append(i)\n","    i += 1\n","\n","val_df = val_df.loc[indices]\n","val_df['labels_task5'] = mv\n","val_df"],"metadata":{"id":"ME5aMz2UUsUu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preprocesado de Datos"],"metadata":{"id":"vJ0309b8aQWE"}},{"cell_type":"markdown","source":["Funciones de limpieza"],"metadata":{"id":"Xn3hA-KeaWbh"}},{"cell_type":"code","source":["import re\n","\n","def remove_links(tweet):\n","    \"\"\"Takes a string and removes web links from it\"\"\"\n","    tweet = re.sub(r'http\\S+', '', tweet)        # remove http links\n","    tweet = re.sub(r'bit.ly/\\S+', '', tweet)     # remove bitly links\n","    tweet = re.sub(r'\\[link\\]', '', tweet )      # remove [link]\n","    tweet = re.sub(r'\\[url\\]', '', tweet )       # remove [url]\n","    tweet = re.sub(r'pic.twitter\\S+','', tweet)\n","    return tweet\n","\n","def remove_users(tweet):\n","    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n","    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove re-tweet\n","    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)      # remove tweeted at\n","    tweet = re.sub(r'\\[user\\]', '', tweet )                      # remove [user]\n","    return tweet\n","\n","def remove_hashtags(tweet):\n","    \"\"\"Takes a string and removes any hash tags\"\"\"\n","    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)      # remove hash tags\n","    return tweet\n","\n","def remove_av(tweet):\n","    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n","    tweet = re.sub('VIDEO:', '', tweet)  # remove 'VIDEO:' from start of tweet\n","    tweet = re.sub('AUDIO:', '', tweet)  # remove 'AUDIO:' from start of tweet\n","    return tweet\n","\n","def remove_emojis(tweet):\n","    emoj = re.compile(\"[\"\n","        u\"\\U00002700-\\U000027BF\"  # Dingbats\n","        u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n","        u\"\\U00002600-\\U000026FF\"  # Miscellaneous Symbols\n","        u\"\\U0001F300-\\U0001F5FF\"  # Miscellaneous Symbols And Pictographs\n","        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n","        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n","        u\"\\U00010000-\\U0010FFFF\"\n","        u\"\\U0001F680-\\U0001F6FF\"  # Transport and Map Symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","        u\"\\U0001f926-\\U0001f937\"\n","        u\"\\U00010000-\\U0010ffff\"\n","        u\"\\u2640-\\u2642\"\n","        u\"\\u2600-\\u2B55\"\n","        u\"\\ufe0f\"  # dingbats\n","\n","                      \"]+\", re.UNICODE)\n","    return re.sub(emoj, '', tweet)"],"metadata":{"id":"DGavid9uaaAv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Aplicación de las funciones"],"metadata":{"id":"1kUxr7Xdah9h"}},{"cell_type":"code","source":["campo_texto = 'text'\n","\n","train_df[campo_texto] = train_df[campo_texto].str.lower()\n","valid_df[campo_texto] = valid_df[campo_texto].str.lower()\n","test_df[campo_texto] = test_df[campo_texto].str.lower()\n","\n","#train_df[campo_texto] = train_df[campo_texto].apply(remove_links)\n","#valid_df[campo_texto] = valid_df[campo_texto].apply(remove_links)\n","#test_df[campo_texto] = test_df[campo_texto].apply(remove_links)\n","\n","#train_df[campo_texto] = train_df[campo_texto].apply(remove_users)\n","#valid_df[campo_texto] = valid_df[campo_texto].apply(remove_users)\n","#test_df[campo_texto] = test_df[campo_texto].apply(remove_users)\n","\n","#train_df[campo_texto] = train_df[campo_texto].apply(remove_hashtags)\n","#valid_df[campo_texto] = valid_df[campo_texto].apply(remove_hashtags)\n","#test_df[campo_texto] = test_df[campo_texto].apply(remove_hashtags)\n","\n","#train_df[campo_texto] = train_df[campo_texto].apply(remove_emojis)\n","#valid_df[campo_texto] = valid_df[campo_texto].apply(remove_emojis)\n","#test_df[campo_texto] = test_df[campo_texto].apply(remove_emojis)\n","\n","train_df"],"metadata":{"id":"IBUKNWhHamRC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Creación de los Modelos"],"metadata":{"id":"tjkZ5vse1Zfg"}},{"cell_type":"code","source":["# Creacion por perspectiva, quedandose con el mayoritario para cada perspectiva\n","etiqueta = 'labels_task5' ###\n","\n","# Cuenta la etiqueta más usada\n","def most_common_label(labels):\n","    counts = {}\n","    for label in labels:\n","        counts[label] = counts.get(label, 0) + 1\n","    return max(counts, key=counts.get)\n","\n","def create_perspectivist_datasets(data, perspective=str):\n","    c = 0\n","    dataset_dict = {}\n","    perspectives = list(set(data[perspective].to_list()[1]))\n","\n","    while c < len(perspectives):\n","        profile = perspectives[c]\n","\n","        df = pd.DataFrame()\n","        ann = 'persp_'+profile\n","        mv = []\n","        indices = []\n","\n","        for i, l in enumerate(data[etiqueta].to_list()):\n","            labels = []\n","            for id, label in enumerate(l):\n","                if data[perspective].to_list()[i][id] == profile:\n","                    if l[id] == 'DIRECT':\n","                        labels.append(1)\n","                    elif l[id] == 'JUDGEMENTAL':\n","                        labels.append(0)\n","            if labels != []:\n","                mas_usado = most_common_label(labels)\n","                mv.append(mas_usado)\n","                indices.append(i)\n","\n","        df['labels'] = mv\n","        df['text'] = [data['text'].to_list()[idx] for idx in indices] # Obtener los textos relevantes\n","\n","        # Balanceo de clases\n","        class_counts = df['labels'].value_counts()\n","        majority_class = class_counts.idxmax()\n","        minority_class = class_counts.idxmin()\n","        majority_count = class_counts[majority_class]\n","        minority_count = class_counts[minority_class]\n","        if majority_count > minority_count:\n","            target_majority_count = int(minority_count * 1)\n","            majority_indices = df[df['labels'] == majority_class].index\n","            balanced_majority_indices = np.random.choice(majority_indices, target_majority_count, replace=False)\n","            balanced_indices = df[df['labels'] == minority_class].index.union(balanced_majority_indices)\n","            df_balanced = df.loc[balanced_indices]\n","        else:\n","            df_balanced = df.copy()\n","\n","        df_balanced.reset_index(drop=True, inplace=True)\n","        dataset_dict[ann] = df_balanced\n","\n","        c += 1\n","\n","    return dataset_dict\n","\n","\n","perspectiva = 'gender_annotators'\n","training_dict_gender = create_perspectivist_datasets(train_df, perspective=perspectiva)\n","\n","perspectiva = 'age_annotators'\n","training_dict_age = create_perspectivist_datasets(train_df, perspective=perspectiva)\n","\n","perspectiva = 'study_levels_annotators'\n","training_dict_study = create_perspectivist_datasets(train_df, perspective=perspectiva)\n","\n","perspectiva = 'ethnicities_annotators'\n","training_dict_ethnicities = create_perspectivist_datasets(train_df, perspective=perspectiva)"],"metadata":{"id":"sEz-Z1S81zD5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Comprobar balanceo de clases\n","training = [training_dict_gender, training_dict_age, training_dict_study, training_dict_ethnicities]\n","\n","for dataset in training:\n","    counts = {}\n","    for persp, df in dataset.items():\n","        counts[persp] = df['labels'].value_counts()\n","\n","    print(counts)\n","    print()"],"metadata":{"id":"skus-eNQ2JEb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Eleccion del modelo\n","model_checkpoint = \"xlm-roberta-base\"\n","#model_checkpoint = 'bert-base-multilingual-uncased'\n","#model_checkpoint = 'annahaz/xlm-roberta-base-misogyny-sexism-indomain-mix-bal'\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"],"metadata":{"id":"09rLeSot2LAx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["datasets_dict = {\n","    'persp_M': {'train': training_dict_gender['persp_M']},\n","    'persp_F': {'train': training_dict_gender['persp_F']},\n","    'persp_23-45': {'train': training_dict_age['persp_23-45']},\n","    'persp_18-22': {'train': training_dict_age['persp_18-22']},\n","    'persp_46+': {'train': training_dict_age['persp_46+']},\n","    'persp_Bachelor’s degree': {'train': training_dict_study['persp_Bachelor’s degree']},\n","    'persp_High school degree or equivalent': {'train': training_dict_study['persp_High school degree or equivalent']},\n","    'persp_White or Caucasian': {'train': training_dict_ethnicities['persp_White or Caucasian']},\n","}\n","\n","# Reseteado de indices\n","for dataset_type in datasets_dict.values():\n","    for dataset in dataset_type.values():\n","        dataset.reset_index(drop=True, inplace=True)\n","\n","\n","datasets_dict"],"metadata":{"id":"ewB73Bj42OLg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nombre_etiqueta = 'labels_task5'\n","\n","def set_labels(records):\n","  if records[nombre_etiqueta] == 'JUDGEMENTAL':\n","    label = 0\n","  else:\n","    label = 1\n","  return {'labels': label}\n","\n","val_df = Dataset.from_pandas(val_df)\n","val_df.reset_format()\n","val_df = val_df.map(set_labels)"],"metadata":{"id":"4cD6e4fO2Orw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Definición de Métricas"],"metadata":{"id":"PnZnwW_5ckK0"}},{"cell_type":"code","source":["# Función para realizar distintas métricas en ejecución\n","\n","def compute_metrics(eval_pred):\n","\n","  ##############\n","  ## predictions son logits, que son tuplas de la forma [valor1, valor2]\n","  ## Por ejemplo [-1.5606991,  1.6122842] significa que ha predicho eso para un documento\n","  ## Eso es lo que pasa a la última capa del transformer (softmax si es binario)\n","  ## Por eso se utiliza el índice del valor máximo de la tupla, para decir que esa es la clase que predice\n","\n","  ## label_ids = [0, 1, 1, 0, 1]  # Etiquetas reales\n","  ## predictions = [\n","  ##  [0.8, 0.2],  # Predicciones para la primera instancia\n","  ##  [0.3, 0.7],  # Predicciones para la segunda instancia\n","  ##  [0.1, 0.9],  # Predicciones para la tercera instancia\n","  ##  [0.9, 0.1],  # Predicciones para la cuarta instancia\n","  ##  [0.4, 0.6],  # Predicciones para la quinta instancia\n","  ##           ]\n","\n","  ##############\n","\n","  labels = eval_pred.label_ids\n","  preds = eval_pred.predictions.argmax(-1)\n","\n","  # Compute precision, recall, F1-score, and support\n","  precision, recall, f1, _ = sk.metrics.precision_recall_fscore_support(labels, preds, average=\"macro\")\n","\n","  # Calculate F1-score for the minority class (label = 1)\n","  f1_minoritaria= f1_score(labels, preds, pos_label=1)\n","\n","  # Calculate F1-score for the majority class (label = 0)\n","  f1_mayoritaria = f1_score(labels, preds, pos_label=0)\n","\n","  # Calculate accuracy\n","  acc = sk.metrics.accuracy_score(labels, preds)\n","\n","  # Calculate Area Under the Curve (AUC)\n","  AUC = roc_auc_score(labels, preds)\n","\n","  # Calculate Precision-Recall Area Under the Curve (AUC)\n","  PREC_REC = average_precision_score(labels, preds)\n","\n","  return {\n","      'accuracy': acc,\n","      'f1': f1,\n","      'precision': precision,\n","      'recall': recall,\n","      'AUC': AUC,\n","      'f1_minoritaria': f1_minoritaria,\n","      'f1_mayoritaria': f1_mayoritaria,\n","      'PREC_REC': PREC_REC\n","  }"],"metadata":{"id":"kGyqeyKUcnEt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Entrenamiento del Modelo"],"metadata":{"id":"LY8NgExbcqHp"}},{"cell_type":"code","source":["# Se carga el modelo preentrenado\n","n_labels = 2\n","\n","# El uso de una función de inicialización facilita la repetición del entrenamiento\n","# Se puede usar la misma función de inicialización en diferentes ejecuciones del código o en configuraciones de entrenamiento diferentes\n","# Esto facilita la repetición del entrenamiento y la reproducibilidad, ya que se puede inicializar el modelo\n","# de la misma manera en cada ejecución.\n","\n","def model_init():\n","    return AutoModelForSequenceClassification.from_pretrained(model_checkpoint,\n","                                                              num_labels = n_labels) #, return_dict = True )\n","                                                              # use_auth_token = 'token propio de HugginFace')"],"metadata":{"id":"Jef8CACGcwej"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Para saber el nombre del modelo\n","model_name = model_checkpoint.split(\"/\")[-1]\n","model_name"],"metadata":{"id":"C7Kq33kgcz2S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fine-tuning"],"metadata":{"id":"HrXKRS2uc1xQ"}},{"cell_type":"code","source":["# Selección de hiperparámetros\n","BATCH_SIZE = 32\n","NUM_TRAIN_EPOCHS = 15\n","LEARNING_RATE = 3e-5\n","MAX_LENGTH = 128\n","WEIGHT_DECAY = 0.1"],"metadata":{"id":"5Pdcw_s4c4SQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenize_data(examples):\n","  return tokenizer(examples[campo_texto], truncation=True, max_length=MAX_LENGTH, padding=True)"],"metadata":{"id":"x2qDF3Vr2lCR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import Trainer, TrainingArguments\n","\n","optim = [\"adamw_hf\", \"adamw_torch\", \"adamw_apex_fused\", \"adafactor\", \"adamw_torch_xla\"]\n","MAX_LENGTH = 128\n","\n","training_args = TrainingArguments(\n","    output_dir = 'results',\n","    num_train_epochs = NUM_TRAIN_EPOCHS,\n","    learning_rate = LEARNING_RATE,\n","    per_device_train_batch_size = BATCH_SIZE,\n","    per_device_eval_batch_size = BATCH_SIZE,\n","    load_best_model_at_end = True,\n","    metric_for_best_model = 'recall', # Cambiar la metrica por la que queremos ajustar\n","    weight_decay = WEIGHT_DECAY,\n","    evaluation_strategy = 'epoch',\n","    save_strategy = 'epoch',\n","    save_total_limit = 3,\n","    optim = optim[1],\n","    push_to_hub = False\n",")\n","\n","output_dir = '/Dataset/Modelos/Bert_Original'\n","\n","# Entrenamiento de cada modelo por separado\n","for task_name, dataset in datasets_dict.items():\n","    model_output_dir = f\"{output_dir}/modelo_{task_name}\"\n","\n","    # Define el conjunto de datos de entrenamiento específico para esta tarea\n","    train_dataset = dataset['train']\n","    valid_dataset = val_df\n","\n","    train_dataset = Dataset.from_pandas(train_dataset)\n","\n","    # Reseteamos el formato para que no haya fallos\n","    train_dataset.reset_format()\n","    valid_dataset.reset_format()\n","\n","    columns_train = train_dataset.column_names  # Coge todas las columnas\n","    columns_valid = valid_dataset.column_names  # Coge todas las columnas\n","    columns_train.remove(\"labels\") # Elimina la columna \"labels\"\n","    columns_valid.remove(\"labels\") # Elimina la columna \"labels\"\n","\n","    # Hace la tokenización y elimina todas las columnas que no se necesitan\n","    encoded_train_dataset = train_dataset.map(tokenize_data, batched=True, remove_columns=columns_train)\n","    encoded_valid_dataset = valid_dataset.map(tokenize_data, batched=True, remove_columns=columns_valid)\n","\n","    # Inicializa el entrenador\n","    trainer = Trainer(\n","        model_init = model_init,\n","        args=training_args,\n","        compute_metrics = compute_metrics,\n","        train_dataset=encoded_train_dataset,\n","        eval_dataset=encoded_valid_dataset,\n","        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n","        tokenizer=tokenizer,\n","    )\n","\n","    # Entrena el modelo\n","    trainer.train()\n","\n","    # Guarda el modelo entrenado\n","    trainer.save_model(model_output_dir)"],"metadata":{"id":"VjWz_8yCc8pD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Evaluación del Modelo"],"metadata":{"id":"juuMrKekdPiG"}},{"cell_type":"markdown","source":["### Lectura de Test"],"metadata":{"id":"rhexgs3xdUQv"}},{"cell_type":"code","source":["# Cargamos los datos de entrenamiento test\n","test_data_path = '/content/drive/MyDrive/Dataset/Task 5/test_task5_hard.json'\n","\n","# Los transformamos en Dataframes\n","test_df = pd.read_json(train_data_path, orient='index')\n","test_df"],"metadata":{"id":"umeFnhdBdWBr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Preprocesado Test"],"metadata":{"id":"Z_zhXoMSdiJd"}},{"cell_type":"code","source":["# Funciones de limpieza\n","import re\n","\n","def remove_links(tweet):\n","    \"\"\"Takes a string and removes web links from it\"\"\"\n","    tweet = re.sub(r'http\\S+', '', tweet)        # remove http links\n","    tweet = re.sub(r'bit.ly/\\S+', '', tweet)     # remove bitly links\n","    tweet = re.sub(r'\\[link\\]', '', tweet )      # remove [link]\n","    tweet = re.sub(r'\\[url\\]', '', tweet )       # remove [url]\n","    tweet = re.sub(r'pic.twitter\\S+','', tweet)\n","    return tweet\n","\n","def remove_users(tweet):\n","    \"\"\"Takes a string and removes retweet and @user information\"\"\"\n","    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove re-tweet\n","    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)      # remove tweeted at\n","    tweet = re.sub(r'\\[user\\]', '', tweet )                      # remove [user]\n","    return tweet\n","\n","def remove_hashtags(tweet):\n","    \"\"\"Takes a string and removes any hash tags\"\"\"\n","    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)      # remove hash tags\n","    return tweet\n","\n","def remove_av(tweet):\n","    \"\"\"Takes a string and removes AUDIO/VIDEO tags or labels\"\"\"\n","    tweet = re.sub('VIDEO:', '', tweet)  # remove 'VIDEO:' from start of tweet\n","    tweet = re.sub('AUDIO:', '', tweet)  # remove 'AUDIO:' from start of tweet\n","    return tweet\n","\n","def remove_emojis(tweet):\n","    emoj = re.compile(\"[\"\n","        u\"\\U00002700-\\U000027BF\"  # Dingbats\n","        u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n","        u\"\\U00002600-\\U000026FF\"  # Miscellaneous Symbols\n","        u\"\\U0001F300-\\U0001F5FF\"  # Miscellaneous Symbols And Pictographs\n","        u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n","        u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n","        u\"\\U00010000-\\U0010FFFF\"\n","        u\"\\U0001F680-\\U0001F6FF\"  # Transport and Map Symbols\n","        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","        u\"\\U00002702-\\U000027B0\"\n","        u\"\\U000024C2-\\U0001F251\"\n","        u\"\\U0001f926-\\U0001f937\"\n","        u\"\\U00010000-\\U0010ffff\"\n","        u\"\\u2640-\\u2642\"\n","        u\"\\u2600-\\u2B55\"\n","        u\"\\ufe0f\"  # dingbats\n","\n","                      \"]+\", re.UNICODE)\n","    return re.sub(emoj, '', tweet)"],"metadata":{"id":"flW_6V50dko1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["campo_texto = 'text'\n","\n","test_df[campo_texto] = test_df[campo_texto].str.lower()\n","#test_df[campo_texto] = test_df[campo_texto].apply(remove_links)\n","#test_df[campo_texto] = test_df[campo_texto].apply(remove_users)\n","#test_df[campo_texto] = test_df[campo_texto].apply(remove_hashtags)\n","#test_df[campo_texto] = test_df[campo_texto].apply(remove_emojis)"],"metadata":{"id":"JQdXk8Amdotq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Lo pasamos a objeto dataset\n","test_dataset = Dataset.from_pandas(test_df)\n","test_dataset"],"metadata":{"id":"e8hJ9QNb4FeI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nombre_etiqueta = 'labels_task5'\n","\n","def set_labels(records):\n","  if records[nombre_etiqueta] == 'JUDGEMENTAL':\n","    label = 0\n","  else:\n","    label = 1\n","  return {'labels': label}\n","\n","# Pasamos la etiqueta a label y le damos formato numérico\n","test_dataset = test_dataset.map(set_labels)  # La función set_labels ya se definió en el entrenamiento\n","test_dataset"],"metadata":{"id":"k03vpBzbdt4V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Cargar Modelos"],"metadata":{"id":"orHDH5fI4d9p"}},{"cell_type":"code","source":["# Se carga los modelos que se han entrenado\n","modelos = [\"persp_M\", \"persp_F\", \"persp_23-45\", \"persp_18-22\", \"persp_46+\", \"persp_Bachelor’s degree\", \"persp_High school degree or equivalent\", \"persp_White or Caucasian\"]\n","\n","model_path = '/content/drive/MyDrive/Dataset/Modelos/Task 5/Modelo1/modelo_'\n","\n","loaded_models = {}\n","for modelo in modelos:\n","    model = AutoModelForSequenceClassification.from_pretrained(model_path + modelo)\n","    loaded_models[modelo] = model"],"metadata":{"id":"k7dVLldl5Dm_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loaded_models"],"metadata":{"id":"8IC-7Urc5Hkv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Realizar Predicciones"],"metadata":{"id":"uY3etOky5L1n"}},{"cell_type":"code","source":["# Añadimos la función de evaluación\n","def compute_metrics(pred):\n","\n","  labels = pred[0]\n","  preds = pred[1]\n","  precision, recall, f1, _ = sk.metrics.precision_recall_fscore_support(labels, preds, average=\"macro\")\n","  acc = sk.metrics.accuracy_score(labels, preds)\n","  AUC = roc_auc_score(labels, preds)\n","  PREC_REC = average_precision_score(labels, preds)\n","  return { 'accuracy': acc, 'f1': f1, 'precision': precision,\n","          'recall': recall, 'AUC': AUC, 'PREC_REC': PREC_REC }"],"metadata":{"id":"fFx27y7A5NY3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Eleccion del modelo\n","#model_checkpoint = \"xlm-roberta-base\"\n","#tokenizer1 = AutoTokenizer.from_pretrained(model_checkpoint)\n","model_checkpoint = 'bert-base-multilingual-uncased'\n","tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n","\n","#tokenizer = [tokenizer1, tokenizer2, tokenizer2, tokenizer1, tokenizer1, tokenizer2, tokenizer1, tokenizer1] # Si es para modelo Bert/Roberta\n","#i = 0\n","\n","# Crear un diccionario de pipelines para cada modelo\n","pipes = {}\n","for modelo_nombre, modelo in loaded_models.items():\n","    #pipe = pipeline(\"text-classification\", model=modelo, tokenizer=tokenizer[i], device=0)\n","    #i += 1\n","    pipe = pipeline(\"text-classification\", model=modelo, tokenizer=tokenizer, device=0)\n","    pipes[modelo_nombre] = pipe\n","\n","# Definir la función para hacer predicciones para todos los modelos a la vez\n","def get_predictions_for_all_models(records):\n","    predictions_all_models = {}\n","    for modelo_nombre, pipe in pipes.items():\n","        result = pipe(records[campo_texto], truncation=True)\n","        pred_label = result[0]['label']\n","        score_label = result[0]['score']\n","\n","        if pred_label == 'LABEL_0':\n","            pred_label = 0\n","        else:\n","            pred_label = 1\n","\n","        predictions_all_models[modelo_nombre] = pred_label\n","\n","    return predictions_all_models\n","\n","# Aplicar la función a los conjuntos de datos de prueba y validación\n","test_dataset_predicted_all_models = test_dataset.map(get_predictions_for_all_models)\n","\n","# Imprimir el primer ejemplo de los conjuntos de datos con predicciones para todos los modelos\n","print(\"Primer ejemplo del conjunto de datos de prueba con predicciones para todos los modelos:\")\n","print(test_dataset_predicted_all_models[0])"],"metadata":{"id":"E_tLwoSh5TJX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataset_predicted_all_models"],"metadata":{"id":"k3SpGM0s5aq3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataset_predicted_all_models.set_format('pandas')\n","df_test = test_dataset_predicted_all_models[:]\n","df_test"],"metadata":{"id":"uw6eWF2r5cTH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluacion de Predicciones Individuales"],"metadata":{"id":"tbDFK0yx5iv3"}},{"cell_type":"code","source":["test_labels = df_test['labels'].values.tolist()\n","\n","# Iterar sobre cada modelo para evaluarlo\n","for modelo_nombre, pipe in pipes.items():\n","    # Hacer predicciones utilizando el modelo en el conjunto de datos de prueba\n","    predicciones = df_test[modelo_nombre].values.tolist()\n","    eval_pred_test = [test_labels, predicciones]\n","\n","    # Calcular las métricas utilizando la función compute_metrics\n","    metrics = compute_metrics(eval_pred_test)\n","\n","    # Imprimir las métricas para el modelo actual\n","    print(f\"Métricas para el modelo '{modelo_nombre}':\")\n","    #print(\"Accuracy:\", metrics['accuracy'])\n","    print(\"F1 score:\", metrics['f1'])\n","    #print(\"Precision:\", metrics['precision'])\n","    #print(\"Recall:\", metrics['recall'])\n","    #print(\"AUC:\", metrics['AUC'])\n","    #print(\"Precision-Recall AUC:\", metrics['PREC_REC'])\n","    print(\"\\n\")"],"metadata":{"id":"xyvdI5Lu5m_H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Prediccion Conjunta"],"metadata":{"id":"V1kOfGsM5t7f"}},{"cell_type":"code","source":["pesos = {\n","    \"persp_M\": {\"Peso\": 1},\n","    \"persp_F\": {\"Peso\": 1},\n","    \"persp_23-45\": {\"Peso\": 1},\n","    \"persp_18-22\": {\"Peso\": 1},\n","    \"persp_46+\": {\"Peso\": 1},\n","    \"persp_Bachelor’s degree\": {\"Peso\": 1},\n","    \"persp_High school degree or equivalent\": {\"Peso\": 1},\n","    \"persp_White or Caucasian\": {\"Peso\": 1},\n","}\n","\n","# Calcula la predicción conjunta\n","def calcular_prediccion_conjunta(df_test, pesos):\n","    suma_ponderada = 0\n","    for modelo in modelos:\n","        peso = pesos[modelo][\"Peso\"]\n","        pred = df_test[modelo]\n","        suma_ponderada += pred * peso\n","\n","    # Divide la suma ponderada por el número de modelos\n","    pred_conjunta = suma_ponderada / len(modelos)\n","\n","    # Aplica el umbral de 0.5 a cada elemento de pred_conjunta\n","    for i in range(len(pred_conjunta)):\n","        if pred_conjunta[i] > 0.5:\n","            pred_conjunta[i] = 1\n","        else:\n","            pred_conjunta[i] = 0\n","\n","    return pred_conjunta\n","\n","# Calculo de predicciones conjuntas\n","predicciones_finales = calcular_prediccion_conjunta(df_test, pesos)\n","\n","# Imprimir las predicciones finales\n","print(\"Predicciones finales:\")\n","predicciones_finales"],"metadata":{"id":"Tx5baKZy5wpg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_labels = df_test['labels'].values.tolist()\n","eval_pred_test = [test_labels, predicciones_finales]\n","\n","# Calcular las métricas utilizando la función compute_metrics\n","metrics = compute_metrics(eval_pred_test)\n","\n","# Imprimir las métricas para el modelo actual\n","print(f\"Métricas para el modelo '{modelo_nombre}':\")\n","print(\"Accuracy:\", metrics['accuracy'])\n","print(\"F1 score:\", metrics['f1'])\n","print(\"Precision:\", metrics['precision'])\n","print(\"Recall:\", metrics['recall'])\n","print(\"AUC:\", metrics['AUC'])\n","print(\"Prec Rec:\", metrics['PREC_REC'])\n","print(\"\\n\")"],"metadata":{"id":"O2aHU8y051n4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Calculo de Balance de Pesos"],"metadata":{"id":"-OK3j0Bq56-v"}},{"cell_type":"code","source":["import itertools\n","\n","# Función para calcular las métricas dado un conjunto de pesos\n","def calcular_metricas_con_pesos(df_test, pesos):\n","    predicciones_finales = calcular_prediccion_conjunta(df_test, pesos)\n","    test_labels = df_test['labels'].values.tolist()\n","    eval_pred_test = [test_labels, predicciones_finales]\n","    metrics = compute_metrics(eval_pred_test)\n","    return metrics\n","\n","# Obtener la lista de modelos del diccionario de pesos\n","modelos = list(pesos.keys())\n","\n","# Obtener todas las combinaciones posibles de pesos que suman 8\n","combinaciones_pesos = []\n","pesos_posibles = [0.5, 0.75, 1, 1.25, 1.5, 1.75]\n","\n","for p in itertools.product(pesos_posibles, repeat=len(modelos)):\n","    if sum(p) == 8:\n","        combinaciones_pesos.append(p)\n","\n","# Inicializar las mejores métricas y pesos\n","mejor_metricas = None\n","mejor_pesos = None\n","it = 0\n","\n","# Iterar sobre todas las combinaciones de pesos\n","for pesos_combinacion in combinaciones_pesos:\n","    pesos = {modelo: {\"Peso\": peso} for modelo, peso in zip(modelos, pesos_combinacion)}\n","    metricas = calcular_metricas_con_pesos(df_test, pesos)\n","    print(it)\n","    it += 1\n","\n","    # Actualizar las mejores métricas y pesos si las métricas actuales son mejores\n","    if mejor_metricas is None or metricas['f1'] > mejor_metricas['f1']:\n","        mejor_metricas = metricas\n","        mejor_pesos = pesos\n","\n","# Imprimir las métricas del mejor modelo\n","print(\"Mejor conjunto de pesos:\")\n","print(mejor_pesos)\n","print(\"Métricas del mejor modelo:\")\n","print(\"Accuracy:\", mejor_metricas['accuracy'])\n","print(\"F1 score:\", mejor_metricas['f1'])\n","print(\"Precision:\", mejor_metricas['precision'])\n","print(\"Recall:\", mejor_metricas['recall'])\n","print(\"AUC:\", mejor_metricas['AUC'])\n","print(\"Prec Rec:\", mejor_metricas['PREC_REC'])"],"metadata":{"id":"4RQY0Jdu5953"},"execution_count":null,"outputs":[]}]}